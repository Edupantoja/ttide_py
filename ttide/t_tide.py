# Autogenerated with SMOP version 0.23
# main.py ../t_tide1.3/t_tide.m -o ../t_tide_py/t_tide.py
from __future__ import division
import numpy as np
from scipy.io import loadmat,savemat
import os
import scipy.interpolate as spi
import scipy.signal as sps
import sys
import matplotlib.mlab as mplm
from t_getconsts import t_getconsts
np.set_printoptions(precision=8,suppress=True)



def t_tide(xin):
    """T_TIDE Harmonic analysis of a time series
     [NAME,FREQ,TIDECON,XOUT]=T_TIDE(XIN) computes the tidal analysis 
     of the (possibly complex) time series XIN.

     [TIDESTRUC,XOUT]=T_TIDE(XIN) returns the analysis information in
     a structure formed of NAME, FREQ, and TIDECON.

     XIN can be scalar (e.g. for elevations), or complex ( =U+sqrt(-1)*V
     for eastward velocity U and northward velocity V.

     Further inputs are optional, and are specified as property/value pairs
     [...]=T_TIDE(XIN,property,value,property,value,...,etc.)

     These properties are:

           'interval'       Sampling interval (hours), default = 1. 

       The next two are required if nodal corrections are to be computed,
       otherwise not necessary. If they are not included then the reported
       phases are raw constituent phases at the central time. 

       If your time series is longer than 18.6 years then nodal corrections
       are not made -instead we fit directly to all satellites (start time
       is then just used to generate Greenwich phases).

           'start time'     [year,month,day,hour,min,sec]
                            - min,sec are optional OR 
                            decimal day (matlab DATENUM scalar)
           'latitude'       decimal degrees (+north) (default: none).

       Where to send the output.
           'output'         where to send printed output:
                            'none'    (no printed output)
                            'screen'  (to screen) - default
                            FILENAME   (to a file)

       Correction factor for prefiltering.
           'prefilt'        FS,CORR
                            If the time series has been passed through
                            a pre-filter of some kind (say, to reduce the
                            low-frequency variability), then the analyzed
                            constituents will have to be corrected for 
                            this. The correction transfer function 
                            (1/filter transfer function) has (possibly 
                            complex) magnitude CORR at frequency FS (cph). 
                            Corrections of more than a factor of 100 are 
                            not applied; it is assumed these refer to tidal
                            constituents that were intentionally filtered 
                            out, e.g., the fortnightly components.

       Adjustment for long-term behavior ("secular" behavior).
           'secular'        'mean'   - assume constant offset (default).
                            'linear' - get linear trend.

       Inference of constituents.
           'inference'      NAME,REFERENCE,AMPRAT,PHASE_OFFSET
                            where NAME is an array of the names of 
                            constituents to be inferred, REFERENCE is an 
                            array of the names of references, and AMPRAT 
                            and PHASE_OFFSET are the amplitude factor and
                            phase offset (in degrees)from the references. 
                            NAME and REFERENCE are Nx4 (max 4 characters
                            in name), and AMPRAT and PHASE_OFFSET are Nx1
                            (for scalar time series) and Nx2 for vector 
                            time series (column 1 is for + frequencies and
                            column 2 for - frequencies).
                            NB - you can only infer ONE unknown constituent
                            per known constituent (i.e. REFERENCE must not 
                            contain multiple instances of the same name).

       Shallow water constituents
           'shallow'        NAME
                            A matrix whose rows contain the names of 
                            shallow-water constituents to analyze.

       Resolution criterions for least-squares fit.        
           'rayleigh'       scalar - Rayleigh criteria, default = 1.
                            Matrix of strings - names of constituents to
                                       use (useful for testing purposes).

       Calculation of confidence limits.
           'error'          'wboot'  - Boostrapped confidence intervals 
                                       based on a correlated bivariate 
                                       white-noise model.
                            'cboot'  - Boostrapped confidence intervals 
                                       based on an uncorrelated bivariate 
                                       coloured-noise model (default).
                            'linear' - Linearized error analysis that 
                                       assumes an uncorrelated bivariate 
                                       coloured noise model. 

       Computation of "predicted" tide (passed to t_predic, but note that
                                        the default value is different).
           'synthesis'      0 - use all selected constituents
                            scalar>0 - use only those constituents with a 
                                       SNR greater than that given (1 or 2 
                                       are good choices, 2 is the default).
                                  <0 - return result of least-squares fit 
                                       (should be the same as using '0', 
                                       except that NaN-holes in original 
                                       time series will remain and mean/trend
                                       are included).

       Least squares soln computational efficiency parameter
    	'lsq'		'direct'  - use A\ x fit
    			'normal'  - use (A'A)\(A'x) (may be necessary
    				    for very large input vectors since
                                       A'A is much smaller than A)
    			'best'	  - automatically choose based on
    				    length of series (default).

           It is possible to call t_tide without using property names,
           in which case the assumed calling sequence is

              T_TIDE(XIN,INTERVAL,START_TIME,LATITUDE,RAYLEIGH)


      OUTPUT: 

        nameu=list of constituents used
        fu=frequency of tidal constituents (cycles/hr)
        tidecon=[fmaj,emaj,fmin,emin,finc,einc,pha,epha] for vector xin
               =[fmaj,emaj,pha,epha] for scalar (real) xin
           fmaj,fmin - constituent major and minor axes (same units as xin)       
           emaj,emin - 95
     confidence intervals for fmaj,fmin
           finc - ellipse orientations (degrees)
           einc - 95
     confidence intervals for finc
           pha - constituent phases (degrees relative to Greenwich)
           epha - 95
     confidence intervals for pha
        xout=tidal prediction

     Note: Although missing data can be handled with NaN, it is wise not
           to have too many of them. If your time series has a lot of 
           missing data at the beginning and/or end, then truncate the 
           input time series.  The Rayleigh criterion is applied to 
           frequency intervals calculated as the inverse of the input 
           series length.

     A description of the theoretical basis of the analysis and some
     implementation details can be found in:

     Pawlowicz, R., B. Beardsley, and S. Lentz, "Classical Tidal 
       "Harmonic Analysis Including Error Estimates in MATLAB 
        using T_TIDE", Computers and Geosciences, 28, 929-937 (2002).

     (citation of this article would be appreciated if you find the
      toolbox useful).
     R. Pawlowicz 11/8/99 - Completely rewritten from the transliterated-
                            to-matlab IOS/Foreman fortran code by S. Lentz
                            and B. Beardsley.
                  3/3/00  - Redid errors to take into account covariances 
                            between u and v errors.
                  7/21/00 - Found that annoying bug in error calc! 
                  11/1/00 - Added linear error analysis.
                  8/29/01 - Made synth=1 default, also changed behavior 
                            when no lat/time given so that phases are raw
                            at central time. 
                  9/1/01  - Moved some SNR code to t_predic.
                  9/28/01 - made sure you can't choose Z0 as constituent.
                  6/12/01 - better explanation for variance calcs, fixed
                            bug in typed output (thanks Mike Cook).
                  8/2/03 - Added block processing for long time series (thanks
                           to Derek Goring).
                  9/2/03 - Beta version of 18.6 year series handling
                  12/2/03 - Bug (x should be xin) fixed thanks to Mike Cook (again!)
                  4/3/11 - Changed (old) psd to (new) pwelch calls, also
                           isfinite for finite.
                  23/3/11 - Corrected my conversion from psd to pwelch, thanks
                           to Dan Codiga and (especially) Evan Haug!

     Version 1.3
     ----------------------Parse inputs-----------------------------------
    """
    ray = 1
    dt = 1
    fid = 1
    stime = np.array([])
    lat = np.array([])
    corr_fs = np.array([0, 1000000.0])
    corr_fac = np.array([1, 1])
    secular = 'mean'
    infiname = np.array([])
    infirefname = np.array([])
    shallownames = np.array([])
    constitnames = np.array([])
    errcalc = 'cboot'
    synth = 2
    lsq = 'best'
    k = 1  
    pi=np.pi


    inn= xin.shape # nargout=2    
    if len(inn) != 1:
        error('Input time series is not a vector')
    xin = xin[:]
    # makes xin a column vector
    nobs = max(xin.shape)
    if lsq[0:3] == 'bes':
        # Set matrix method if auto-choice.
        if nobs > 10000:
            lsq = 'normal'
        else:
            lsq = 'direct'
    if np.dot(nobs, dt) > np.dot(np.dot(18.6, 365.25), 24):
        # Long time series
        longseries = 1
        ltype = 'full'
    else:
        longseries = 0
        ltype = 'nodal'
    nobsu = nobs - np.remainder(nobs - 1, 2)
    # makes series odd to give a center point
    t = np.dot(dt, (np.hstack([range(1, (nobs +1))]).reshape(1, -1).T - np.ceil(nobsu / 2)))

    #t = np.dot(dt, (np.arange(nobs).T+1 - np.ceil(nobsu / 2)))
    # Time vector for entire time series,
    # centered at series midpoint. 
    if not  (0 in stime.shape):
        centraltime = stime + np.dot(floor(nobsu / 2) / 24.0, dt)
    else:
        centraltime = np.array([])
    # -------Get the frequencies to use in the harmonic analysis-----------
    nameu, fu, ju, namei, fi, jinf, jref = constituents(ray / (np.dot(dt, nobsu)), constitnames, shallownames, infiname, infirefname, centraltime) # nargout=7
    mu = max(fu.shape)
    # # base frequencies
    mi = max(fi.shape)
    # # inferred
    # Find the good data points (here I assume that in a complex time 
    # series, if u is bad, so is v).
    gd = np.flatnonzero(np.isfinite(xin[0:nobsu]))
    ngood = max(gd.shape)
    #cout
    #print('   Points used: 
    #d of 
    #d\n',ngood,nobs)
    # Now solve for the secular trend plus the analysis. Instead of solving
    # for + and - frequencies using exp(i*f*t), I use sines and cosines to 
    # keep tc real.  If the input series is real, than this will 
    # automatically use real-only computation (faster). However, for the analysis, 
    # it's handy to get the + and - frequencies ('ap' and 'am'), and so 
    # that's what we do afterwards.
    # The basic code solves the matrix problem Ac=x+errors where the functions to
    # use in the fit fill up the A matrix, which is of size (number points)x(number
    # constituents). This can get very, very large for long time series, and
    # for this the more complex block processing algorithm was added. It should
    # give identical results (up to roundoff error)
    if lsq[0:3] == 'dir':
        if secular[0:3] == 'lin':
            tc = np.hstack((np.ones(shape=(max(t.shape), 1), dtype='float64'), np.cos(2*pi*np.dot(t,fu.T)), np.sin(2*pi*np.dot(t,fu.T)),t*(2 / dt / nobsu)))
        else: 
            tc = np.hstack((np.ones(shape=(max(t.shape), 1), dtype='float64'), np.cos(2*pi*np.dot(t,fu.reshape(-1,1).T)), np.sin(2*pi*np.dot(t,fu.reshape(-1,1).T)) ))
        coef = np.linalg.lstsq(tc[gd, :], xin[gd])[0]        

        coef=coef.T
        z0 = coef[0]
        ap = (coef[1:mu+1]-1j*coef[(1+mu):(mu*2)+1])/2   
        # a+ amplitudes
        am = (coef[1:mu+1]+1j*coef[(1+mu):(mu*2)+1])/2
        # a- amplitudes
        if secular[0:3] == 'lin':
            dz0 = coef[-1]
        else:
            dz0 = 0
        xout = np.dot(tc,coef)
        # This is the time series synthesized from the analysis
    else:
        # More complicated code required for long time series when memory may be
        # a problem. Modified from code submitted by Derek Goring (NIWA Chrischurch)
        # Basically the normal equations are formed (rather than using Matlab's \
        # algorithm for least squares); this can be done by adding up subblocks
        # of data. Notice how the code is messier, and we have to recalculate everything
        # to get the original fit.
        nsub = 5000
        # Block length - doesn't matter really but should be small enough to
        # get allocated quickly	      
        if secular[0:3] == 'lin':
            lhs = np.zeros(shape=(np.dot(mu, 2) + 2, np.dot(mu, 2) + 2), dtype='float64')
            rhs = np.zeros(shape=(np.dot(mu, 2) + 2, 1), dtype='float64')
            for j1 in range(1, (ngood +1), nsub):
                j2 = np.min(j1 + nsub - 1, ngood)
                E = np.hstack([np.ones(shape=(j2 - j1 + 1, 1), dtype='float64'), cos(np.dot(np.dot((np.dot(2, pi)), t[(gd[(j1 -1):j2] -1)]), fu.T)), sin(np.dot(np.dot((np.dot(2, pi)), t[(gd[(j1 -1):j2] -1)]), fu.T)), np.dot(t[(gd[(j1 -1):j2] -1)], (2 / dt / nobsu))])
                rhs = rhs + np.dot(E.T, xin[(gd[(j1 -1):j2] -1)])
                lhs = lhs + np.dot(E.T, E)
        else:
            lhs = np.zeros(shape=(np.dot(mu, 2) + 1, np.dot(mu, 2) + 1), dtype='float64')
            rhs = np.zeros(shape=(np.dot(mu, 2) + 1, 1), dtype='float64')
            for j1 in range(1, (ngood +1), nsub):
                j2 = np.min(j1 + nsub - 1, ngood)
                E = np.hstack([np.ones(shape=(j2 - j1 + 1, 1), dtype='float64'), cos(np.dot(np.dot((np.dot(2, pi)), t[(gd[(j1 -1):j2] -1)]), fu.T)), sin(np.dot(np.dot((np.dot(2, pi)), t[(gd[(j1 -1):j2] -1)]), fu.T))])
                rhs = rhs + np.dot(E.T, xin[(gd[(j1 -1):j2] -1)])
                lhs = lhs + np.dot(E.T, E)
        coef = np.linalg.lstsq(lhs, rhs)
        coef= np.vstack((coef[0],coef[3]))
        coef=coef.T
        z0 = coef[0,0]
        ap = (coef[1:mu+1,0]+1j*coef[(1+mu):(mu*2)+1,0])/2   
        # a+ amplitudes
        am = (coef[1:mu+1,0]-1j*coef[(1+mu):(mu*2)+1,0])/2
        # a- amplitudes
        # a- amplitudes
        if secular[0:3] == 'lin':
            dz0 = coef[-1,0]
        else:
            dz0 = 0
        xout = xin
        # Copies over NaN   
        if secular[0:3] == 'lin':
            for j1 in range(1, (nobs +1), nsub):
                j2 = np.min(j1 + nsub - 1, nobs)
                E = np.hstack([np.ones(shape=(j2 - j1 + 1, 1), dtype='float64'), cos(np.dot(np.dot((np.dot(2, pi)), t[(j1 -1):j2]), fu.T)), sin(np.dot(np.dot((np.dot(2, pi)), t[(j1 -1):j2]), fu.T)), np.dot(t[(j1 -1):j2], (2 / dt / nobsu))])
                xout[(j1 -1):j2] = np.dot(E, coef)
        else:
            for j1 in range(1, (nobs +1), nsub):
                j2 = np.min(j1 + nsub - 1, nobs)
                E = np.hstack([np.ones(shape=(j2 - j1 + 1, 1), dtype='float64'), cos(np.dot(np.dot((np.dot(2, pi)), t[(j1 -1):j2]), fu.T)), sin(np.dot(np.dot((np.dot(2, pi)), t[(j1 -1):j2]), fu.T))])            
                xout[(j1 -1):j2] = np.dot(E, coef)
    # Check variance explained (but do this with the original fit).
 
    xres = xin-xout
    # and the residuals!
    if np.isreal(xin).any():
        # Real time series
        varx = np.cov(xin[(gd)])
        varxp = np.cov(xout[(gd)])
        varxr = np.cov(xres[(gd)])
        #cout
        #print('   percent of var residual after lsqfit/var original: 
        #5.2f 
        #\n',100*(varxr/varx));  
    else:
        # Complex time series
        varx = np.cov(real(xin[(gd)]))
        varxp = np.cov(real(xout[(gd)]))
        varxr = np.cov(real(xres[(gd)]))
        #cout
        #print('   percent of X var residual after lsqfit/var original: 
        #5.2f 
        #\n',100*(varxr/varx));
        vary = np.cov(imag(xin[(gd)]))
        varyp = np.cov(imag(xout[(gd)]))
        varyr = np.cov(imag(xres[(gd)]))
        #cout
        #print('   percent of Y var residual after lsqfit/var original: 
        #5.2f 
        #\n',100*(varyr/vary));
    #---------- Correct for prefiltering-----------------------------------
    corrfac = spi.interpolate.interp1d(corr_fs,corr_fac)(fu)
    # To stop things blowing up!
    corrfac[corrfac > 100] = 1
    corrfac[corrfac < 0.01] = 1
    corrfac[np.isnan(corrfac)] = 1
    ap = ap*np.squeeze(corrfac)
    am = am * np.squeeze(np.conj(corrfac))
    #---------------Nodal Corrections-------------------------------------- 						   
    # Generate nodal corrections and calculate phase relative to Greenwich. 						   
    # Note that this is a slightly weird way to do the nodal corrections,							   
    # but is 'traditional'.  The "right" way would be to change the basis							   
    # functions used in the least-squares fit above.									   
    if lat.size !=0 & stime.size !=0:
        # Time and latitude								   
        # Get nodal corrections at midpoint time.										   
        v, u, f = t_vuf(ltype, centraltime, np.array([ju, jinf]).reshape(1, -1), lat) # nargout=3
        vu = np.dot((v + u), 360)
        # total phase correction (degrees)
        nodcor = 'Greenwich phase computed with nodal corrections applied to amplitude \\n and phase relative to center time'
    else:
        if not  (0 in stime.shape):
            # Time only  										   
            # Get nodal corrections at midpoint time										   
            v, u, f = t_vuf(ltype, centraltime, np.array([ju, jinf]).reshape(1, -1)) # nargout=3
            vu = np.dot((v + u), 360)
            # total phase correction (degrees)	
            nodcor = 'Greenwich phase computed, no nodal corrections'
        else:
            # No time, no latitude												   
            vu = np.zeros(shape=(max(ju.shape) + max(jinf.shape), 1), dtype='float64')
            f = np.ones(shape=(max(ju.shape) + max(jinf.shape), 1), dtype='float64')
            nodcor = 'Phases at central time'
    #cout									   
    #print(['   ',nodcor,'\n']);												   
    #---------------Inference Corrections----------------------------------
    # Once again, the "right" way to do this would be to change the basis
    # functions.
    ii = np.flatnonzero(np.isfinite(jref))
    if ii:
        print('   Do inference corrections\\n')
        snarg = np.dot(np.dot(np.dot(nobsu, pi), (fi[(ii -1)] - fu[(jref[(ii -1)] -1)])), dt)
        scarg = sin(snarg) / snarg
        if infamprat.shape[1] == 1:
            # For real time series
            pearg = np.dot(np.dot(2, pi), (vu[(mu + ii -1)] - vu[(jref[(ii -1)] -1)] + infph[(ii -1)])) / 360
            pcfac = infamprat[(ii -1)] * f[(mu + ii -1)] / f[(jref[(ii -1)] -1)] * exp(np.dot(i, pearg))
            pcorr = 1 + pcfac * scarg
            mcfac = conj(pcfac)
            mcorr = conj(pcorr)
        else:
            # For complex time series
            pearg = np.dot(np.dot(2, pi), (vu[(mu + ii -1)] - vu[(jref[(ii -1)] -1)] + infph[(ii -1), 0])) / 360
            pcfac = infamprat[(ii -1), 0] * f[(mu + ii -1)] / f[(jref[(ii -1)] -1)] * exp(np.dot(i, pearg))
            pcorr = 1 + pcfac * scarg
            mearg = np.dot(np.dot(- 2, pi), (vu[(mu + ii -1)] - vu[(jref[(ii -1)] -1)] + infph[(ii -1), 1])) / 360
            mcfac = infamprat[(ii -1), 1] * f[(mu + ii -1)] / f[(jref[(ii -1)] -1)] * exp(np.dot(i, mearg))
            mcorr = 1 + mcfac * scarg
        ap[(jref[(ii -1)] -1)] = ap[(jref[(ii -1)] -1)] / pcorr
        # Changes to existing constituents
        ap = np.array([ap, ap[(jref[(ii -1)] -1)] * pcfac]).reshape(1, -1)
        # Inferred constituents
        am[(jref[(ii -1)] -1)] = am[(jref[(ii -1)] -1)] / mcorr
        am = np.array([am, am[(jref[(ii -1)] -1)] * mcfac]).reshape(1, -1)
        fu = np.array([fu, fi[(ii -1)]]).reshape(1, -1)
        nameu = np.array([nameu, namei[(ii -1), :]]).reshape(1, -1)
    # --------------Error Bar Calculations---------------------------------
    #
    # Error bar calcs involve two steps:
    #      1) Estimate the uncertainties in the analyzed amplitude
    #         for both + and - frequencies (i.e., in 'ap' and 'am').
    #         A simple way of doing this is to take the variance of the
    #         original time series and divide it into the amount appearing
    #         in the bandwidth of the analysis (approximately 1/length).
    #         A more sophisticated way is to assume "locally white"
    #         noise in the vicinity of, e.g., the diurnal consistuents.
    #         This takes into account slopes in the continuum spectrum.
    #
    #      2) Transform those uncertainties into ones suitable for ellipse
    #         parameters (axis lengths, angles). This can be done 
    #         analytically for large signal-to-noise ratios. However, the 
    #         transformation is non-linear at lows SNR, say, less than 10
    #         or so.
    #
    xr = fixgaps(xres)
    # Fill in "internal" NaNs with linearly interpolated
    # values so we can fft things.
    nreal = 1
    if errcalc.endswith('boot'):
        #cout									   
        #print('   Using nonlinear bootstrapped error estimates\n');
        # "noise" matrices are created with the right covariance structure
        # to add to the analyzed components to create 'nreal' REPLICATES. 
        # 
        nreal = 300
        # Create noise matrices 
        NP, NM = noise_realizations(xr[(np.isfinite(xr))], fu, dt, nreal, errcalc) # nargout=2

        # All replicates are then transformed (nonlinearly) into ellipse 
        # parameters.  The computed error bars are then based on the std
        # dev of the replicates. 
        AP = np.repeat(ap,nreal).reshape(len(ap),nreal) + NP
        # Add to analysis (first column
        AM = np.repeat(am,nreal).reshape(len(am),nreal) + NM
        # of NM,NP=0 so first column of
        # AP/M holds ap/m).

        epsp = (np.dot(np.angle(AP), 180) / pi)
        # Angle/magnitude form:
        epsm = (np.dot(np.angle(AM), 180) / pi)
        ap = abs(AP)
        am = abs(AM)
    else:
        if errcalc=='linear':
            print('   Using linearized error estimates\\n')
            #
            # Uncertainties in analyzed amplitudes are computed in different
            # spectral bands. Real and imaginary parts of the residual time series
            # are treated separately (no cross-covariance is assumed).
            #
            # Noise estimates are then determined from a linear analysis of errors,
            # assuming that everything is uncorrelated. This is OK for scalar time
            # series but can fail for vector time series if the noise is not 
            # isotropic.
            ercx, eicx = noise_stats(xr[(isfinite(xr) -1)], fu, dt) # nargout=2
            # Note - here we assume that the error in the cos and sin terms is 
            # equal, and equal to total power in the encompassing frequency bin. 
            # It seems like there should be a factor of 2 here somewhere but it 
            # only works this way! <shrug>
            emaj, emin, einc, epha = errell(ap + am, np.dot(i, (ap - am)), ercx, ercx, eicx, eicx) # nargout=4

            epsp = np.dot(angle(ap), 180) / pi
            epsm = np.dot(angle(am), 180) / pi
            ap = abs(ap)
            am = abs(am)
        else:
            print "Unrecognized type of error analysis: " + errcalc + " specified!"
    #-----Convert complex amplitudes to standard ellipse parameters--------
    aap = ap / f[:]
    # Apply nodal corrections and
    aam = am / f[:]
    # compute ellipse parameters.
    fmaj = aap + aam
    # major axis
    fmin = aap - aam
    # minor axis

    gp = np.mod(vu[:] - epsp, 360)
    # pos. Greenwich phase in deg.
    gm = np.mod(vu[:] + epsm, 360)
    # neg. Greenwich phase in deg.
    finc = (epsp + epsm) / 2
    finc[:, 0] = np.mod(finc[:, 0], 180)
    # Ellipse inclination in degrees
    # (mod 180 to prevent ambiguity, i.e., 
    # we always ref. against northern 
    # semi-major axis.
    finc = cluster(finc, 180)
    # Cluster angles around the 'true' 
    # angle to avoid 360 degree wraps.
    pha = np.mod(gp + finc, 360)
    # Greenwich phase in degrees.
    pha = cluster(pha, 360)

    # Cluster angles around the 'true' angle
    # to avoid 360 degree wraps.
    #----------------Generate 95
    # CI---------------------------------------
    # For bootstrapped errors, we now compute limits of the distribution.
    if errcalc.endswith('boot'):
        # std dev-based estimates.
        # The 95
        # CI are computed from the sigmas
        # by a 1.96 fudge factor (infinite degrees of freedom).
        # emaj=1.96*std(fmaj,0,2);
        # emin=1.96*std(fmin,0,2);
        # einc=1.96*std(finc,0,2);
        # epha=1.96*std(pha ,0,2);
        # Median-absolute-deviation (MAD) based estimates.
        # (possibly more stable?)

        emaj = np.dot(np.median(abs(fmaj - np.dot(np.median(fmaj, 1).reshape(len(np.median(fmaj, 1)),1), np.ones(shape=(1, nreal), dtype='float64'))), 1) / 0.6375, 1.96)
        emin = np.dot(np.median(abs(fmin - np.dot(np.median(fmin, 1).reshape(len(np.median(fmin, 1)),1), np.ones(shape=(1, nreal), dtype='float64'))), 1) / 0.6375, 1.96)
        einc = np.dot(np.median(abs(finc - np.dot(np.median(finc, 1).reshape(len(np.median(finc, 1)),1), np.ones(shape=(1, nreal), dtype='float64'))), 1) / 0.6375, 1.96)
        epha = np.dot(np.median(abs(pha - np.dot(np.median(pha, 1).reshape(len(np.median(pha, 1)),1), np.ones(shape=(1, nreal), dtype='float64'))), 1) / 0.6375, 1.96)
    else:
        # In the linear analysis, the 95
        # CI are computed from the sigmas
        # by this fudge factor (infinite degrees of freedom).
        emaj = np.dot(1.96, emaj)
        emin = np.dot(1.96, emin)
        einc = np.dot(1.96, einc)
        epha = np.dot(1.96, epha)
    if np.isreal(xin).all():
        tidecon = np.array([fmaj[:, 0], emaj, pha[:, 0], epha]).T
    else:
        tidecon = np.array([fmaj[:, 0], emaj, fmin[:, 0], emin, finc[:, 0], einc, pha[:, 0], epha]).T
    # Sort results by frequency (needed if anything has been inferred since 
    # these are stuck at the end of the list by code above).
    if any(np.isfinite(jref)):
        fu, I = sort(fu) # nargout=2
        nameu = nameu[(I -1), :]
        tidecon = tidecon[(I -1), :]
    snr = (tidecon[:, 0] / tidecon[:, 1]) ** 2
    # signal to noise ratio
    #--------Generate a 'prediction' using significant constituents----------
    xoutOLD = xout
    if synth >= 0:
        if lat.size !=0 & stime.size !=0:
            #cout									   
            #print('   Generating prediction with nodal corrections, SNR is 
            #f\n',synth);
            xout = t_predic(stime + np.dot(np.array([range(0, (nobs - 1 +1))]).reshape(1, -1), dt) / 24.0, nameu, fu, tidecon, 'lat', lat, 'synth', synth, 'anal', ltype)
        else:
            if not  (0 in stime.shape):
                #cout									   
                #print('   Generating prediction without nodal corrections, SNR is 
                #f\n',synth);
                xout = t_predic(stime + np.dot(np.array([range(0, (nobs - 1 +1))]).reshape(1, -1), dt) / 24.0, nameu, fu, tidecon, 'synth', synth, 'anal', ltype)
            else:
                #cout									   
                #print('   Generating prediction without nodal corrections, SNR is 
                #f\n',synth);
                print "This would be a t_predic call."
                #haven't fixed t_predic yet commenting it out to to see if t_tide can be made to work.
                #xout = t_predic(t / 24.0, nameu, fu, tidecon, 'synth', synth, 'anal', ltype)
    else:
        print('   Returning fitted prediction\\n')
    # Check variance explained (but now do this with the synthesized fit).
    xres = xin[:] - xout[:]
    # and the residuals!
    #error;
    if np.isreal(xin).all():
        # Real time series
        varx = np.cov(xin[(gd)])
        varxp = np.cov(xout[(gd)])
        varxr = np.cov(xres[(gd)])
        #cout									   
        #print('   percent of var residual after synthesis/var original: 
        #5.2f 
        #\n',100*(varxr/varx));  
    else:
        # Complex time series
        varx = cov(real(xin[(gd)]))
        varxp = cov(real(xout[(gd)]))
        varxr = cov(real(xres[(gd)]))
        #cout									   
        #print('   percent of X var residual after synthesis/var original: 
        #5.2f 
        #\n',100*(varxr/varx));
        vary = cov(imag(xin[(gd)]))
        varyp = cov(imag(xout[(gd)]))
        varyr = cov(imag(xres[(gd)]))
        #cout									   
        #print('   percent of Y var residual after synthesis/var original: 
        #5.2f 
        #\n',100*(varyr/vary));
    #-----------------Output results---------------------------------------
    if fid > 1:
        print  '\\n\n %s\\n', 'file name: ' + filen
    else:
        if fid == 1:
            print  '-----------------------------------'
    if fid > 0:
        date='Placeholder'
        print  'date: %s' % date
        print  'nobs = %d \nngood = %d \nrecord length (days) = %.2f' % (nobs, ngood, np.dot(max(xin.shape), dt) / 24)
        if stime.size != 0:
            print  '\n %s\\n', 'start time: ' + datestr(stime)
        print  'rayleigh criterion = %.1f\n' % ray
        print  '%s\n' % nodcor
        #  print '\n     coefficients from least squares fit of x\n');
        #  print '\n tide    freq        |a+|       err_a+      |a-|       err_a-\n');
        #  for k=1:length(fu);
        #    if ap(k)>eap(k) | am(k)>eam(k), print('*'); else print(' '); end;
        #    print '
        #s  
        #8.5f  
        #9.4f  
        #9.4f  
        #9.4f  
        #9.4f\n',nameu(k,:),fu(k),ap(k),eap(k),am(k),eam(k));
        #  end
        print  'x0= %.3g, x trend= %.3g' % ( np.real(z0), np.real(dz0))
        print  'var(x)= ' , varx , '   var(xp)= ' , varxp , '   var(xres)= ' , varxr , ''
        print ''
        print  'percent var predicted/var original= %.1f ' % (np.dot(100, varxp) / varx)
        print ''
        if np.isreal(xin).all():
            print  '     tidal amplitude and phase with 95 % CI estimates'
            print  '   tide      freq         amp     amp_err     pha     pha_err    snr'
            for k in range(0, max(fu.shape) ):
                if snr[(k)] > synth:
                    print  '* %s  %9.7f  %9.4f  %8.3f  %8.2f  %8.2f  %8.2g' % (nameu[(k)], fu[(k)].astype(float), tidecon[(k), 0].astype(float), tidecon[(k), 1].astype(float), tidecon[(k), 2].astype(float), tidecon[(k), 3].astype(float), snr[(k)].astype(float))
                else:
                   print  '  %s  %9.7f  %9.4f  %8.3f  %8.2f  %8.2f  %8.2g' % (nameu[(k)], fu[(k)].astype(float), tidecon[(k), 0].astype(float), tidecon[(k), 1].astype(float), tidecon[(k), 2].astype(float), tidecon[(k), 3].astype(float), snr[(k)].astype(float))                
        else:
            print  '\\ny0= \n %.3g, x trend= \n %.3g\\n', imag(z0), imag(dz0)
            print  '\\nvar(y)= ' + num2str(vary) + '    var(yp)= ' + num2str(varyp) + '  var(yres)= ' + num2str(varyr) + '\\n'
            print  'percent var predicted/var original= \n %.1f \n %\\n', np.dot(100, varyp) / vary
            print  '\\n\n %s\\n', 'ellipse parameters with 95\n % CI estimates'
            print  '\\n\n %s\\n', 'tide   freq      major  emaj    minor   emin     inc    einc     pha    epha      snr'
            for k in range(1, (max(fu.shape) +1)):
                if snr[(k -1)] > synth:
                    print  '*'
                else:
                    print  ' '
                print  '\n %s \n %9.7f \n %6.3f \n %7.3f \n %7.3f \n %6.2f \n %8.2f \n %6.2f \n %8.2f \n %6.2f \n %6.2g\\n', nameu[(k -1), :], fu[(k -1)], tidecon[(k -1), :], snr[(k -1)]
            print  '\\ntotal var= ' + num2str(varx + vary) + '   pred var= ' + num2str(varxp + varyp) + '\\n'
            print  'percent total var predicted/var original= \n %.1f \n %\\n\\n', np.dot(100, (varxp + varyp)) / (varx + vary)
        if fid != 1:
            st = fid.close()
    xout = xout.reshape(inn[0], 1)
    #if [0, 3, 4] == nargout:
    #    pass
    #else:
    #    if [1] == nargout:
    #        nameu = type('struct', (), {})()
    #    else:
    #        if [2] == nargout:
    #            nameu = type('struct', (), {})()
    #            fu = xout
    return nameu, fu, tidecon, xout
def constituents(minres, constit, shallow, infname, infref, centraltime):
    """[name,freq,kmpr]=constituents(minres,infname) loads tidal constituent
     table (containing 146 constituents), then picks out only the '
     resolvable' frequencies (i.e. those that are MINRES apart), base on 
     the comparisons in the third column of constituents.dat. Only 
     frequencies in the 'standard' set of 69 frequencies are actually used.
     Also return the indices of constituents to be inferred.
     If we have the mat-file, read it in, otherwise create it and read
     it in!
     R Pawlowicz 9/1/01 
     Version 1.0

        19/1/02 - typo fixed (thanks to  Zhigang Xu)
     Compute frequencies from astronomical considerations.
    """
    if minres > 1 / (np.dot(np.dot(18.6, 365.25), 24)):
        # Choose only resolveable pairs for short
        const, sat, cshallow = t_getconsts(centraltime) # nargout=3
        # Time series  
        ju = np.flatnonzero(const['df'] >= minres)
    else:
        # Choose them all if > 18.6 years.
        const, sat, cshallow = t_get18consts(centraltime) # nargout=3
        ju = np.array([range(2, (max(const['freq'].shape) +1))]).reshape(1, -1).T
        # Skip Z0
        for ff in range(1, 3):
            # loop twice to make sure of neightbouring pairs
            jck = np.flatnonzero(diff(const['freq'][ju]) < minres)
            if (max(jck.shape) > 0):
                jrm = jck
                jrm = jrm + (abs(const['doodsonamp'][ju[(jck + 1 -1)]]) < abs(const['doodsonamp'][ju[(jck -1)]]))
                disp('  Warning! Following constituent pairs violate Rayleigh criterion')
#               for ick in range(1, (max(jck.shape) +1)):
#                    disp('     ' + const.name(ju[(jck[(ick -1)] -1)], :) + ' vs ' + const.name(ju[(jck[(ick -1)] + 1 -1)], :) + ' - not using ' + const.name(ju[(jrm[(ick -1)] -1)], :))
                ju[(jrm -1)] = np.array([])
    if constit.size !=0:
        # Selected if constituents are specified in input.
        ju = np.array([])
        for k in range(1, (constit.shape[0] +1)):
            j1 = strmatch(constit[(k -1), :], const['name'])
            if (0 in j1.shape):
                disp("Can't recognize name " + constit[(k -1), :] + ' for forced search')
            else:
                if j1 == 1:
                    disp('*************************************************************************')
                    disp("Z0 specification ignored - for non-tidal offsets see 'secular' property")
                    disp('*************************************************************************')
                else:
                    ju = np.array([ju, j1]).reshape(1, -1)
        dum, II = sort(const['freq'][ju]) # nargout=2
        # sort in ascending order of frequency.
        ju = ju[(II -1)]
    #cout
    #disp(['   number of standard constituents used: ',int2str(length(ju))])
    if shallow.size !=0:
        # Add explictly selected shallow water constituents.
        for k in range(1, (shallow.shape[0] +1)):
            j1 = strmatch(shallow[(k -1), :], const['name'])
            if (0 in j1.shape):
                disp("Can't recognize name " + shallow[(k -1), :] + ' for forced search')
            else:
                if np.isnan(const['ishallow'][j1]):
                    disp(shallow[(k -1), :] + ' Not a shallow-water constituent')
                disp('   Forced fit to ' + shallow[(k -1), :])
                ju = np.array([ju, j1]).reshape(1, -1)
    nameu = const['name'][ju]
    fu = const['freq'][ju]
    # Check if neighboring chosen constituents violate Rayleigh criteria.
    jck = np.flatnonzero(np.diff(fu) < minres)
    #cout
    #if (length(jck)>0)
    #disp(['  Warning! Following constituent pairs violate Rayleigh criterion']);
    #for ick=1:length(jck);
    #disp(['     ',nameu(jck(ick),:),'  ',nameu(jck(ick)+1,:)]);
    #end;
    #end
    # For inference, add in list of components to be inferred.
    fi = np.array([])
    namei = np.array([])
    jinf = np.array([])
    jref = np.array([])
    if infname.size !=0:
        fi = np.zeros(shape=(infname.shape[0], 1), dtype='float64')
        namei = np.zeros(shape=(infname.shape[0], 4), dtype='float64')
        jinf = np.zeros(shape=(infname.shape[0], 1), dtype='float64') + NaN
        jref = np.zeros(shape=(infname.shape[0], 1), dtype='float64') + NaN
        for k in range(1, (infname.shape[0] +1)):
            j1 = strmatch(infname[(k -1), :], const.name)
            if (0 in j1.shape):
                disp("Can't recognize name" + infname[(k -1), :] + ' for inference')
            else:
                jinf[(k -1)] = j1
                fi[(k -1)] = const['freq'][j1]
                namei[(k -1), :] = const['name'][j1, :]
                j1 = strmatch(infref[(k -1), :], nameu)
                if (0 in j1.shape):
                    disp("Can't recognize name " + infref[(k -1), :] + ' for as a reference for inference')
                else:
                    jref[(k -1)] = j1
                    print('   Inference of ' + namei[(k -1), :] + ' using ' + nameu[(j1 -1), :] + '\\n')
        jinf[(isnan(jref) -1)] = NaN
    return nameu, fu, ju, namei, fi, jinf, jref
def fixgaps(x):
    """FIXGAPS: Linearly interpolates gaps in a time series
     YOUT=FIXGAPS(YIN) linearly interpolates over NaN in the input time 
     series (may be complex), but ignores trailing and leading NaNs.
     R. Pawlowicz 11/6/99
     Version 1.0
    """
    y = x
    bd = np.isnan(x)
    gd = np.flatnonzero(~bd)
    idx=np.array([range(1, ((np.min(gd) - 1) +1)), range((np.max(gd) + 1), (len(gd)))]).reshape(1, -1) -1
    if idx.size !=0:
        bd[idx] = 0
        y[(bd -1)] = interp1(gd, x[(gd -1)], np.flatnonzero(bd))
    
    return y
def cluster(ain, clusang):
    """CLUSTER: Clusters angles in rows around the angles in the first 
     column. CLUSANG is the allowable ambiguity (usually 360 degrees but
     sometimes 180).
    """
    makearray=(ain - np.repeat(ain[:,1],ain.shape[1]).reshape(ain.shape[0],ain.shape[1]))
    ii = makearray> clusang / 2
    ain[(ii)] = ain[(ii)] - clusang
    ii = (ain - np.repeat(ain[:,1],ain.shape[1]).reshape(ain.shape[0],ain.shape[1])) < - clusang / 2
    ain[(ii)] = ain[(ii)] + clusang
    return ain
def noise_realizations(xres, fu, dt, nreal, errcalc):
    """NOISE_REALIZATIONS: Generates matrices of noise (with correct
     cross-correlation structure) for bootstrap analysis.

     R. Pawlowicz 11/10/00
     Version 1.0
    """
    if errcalc=='cboot':
        fband, Pxrave, Pxiave, Pxcave = residual_spectrum(xres, fu, dt) # nargout=4
        aaa,bbb=Pxcave.shape
        Pxcave = np.zeros((aaa, bbb), dtype='float64')
        # For comparison with other technique!
        #print('**** Assuming no covariance between u and v errors!*******\n');
    else:
        if errcalc=='wboot':
            fband = np.array([0, 0.5]).reshape(1, -1)
            nx = max(xres.shape)
            A = cov(real(xres), imag(xres)) / nx
            Pxrave = A[0, 0]
            Pxiave = A[1, 1]
            Pxcave = A[0, 1]
        else:
            sys.exit("Unrecognized type of bootstap analysis specified: '" + errcalc + "'")
    nfband = fband.shape[0]
    Mat = np.zeros(shape=(4, 4, nfband), dtype='float64')
    for k in range(1, (nfband +1)):
        # The B matrix represents the covariance matrix for the vector
        # [Re{ap} Im{ap} Re{am} Im{am}]' where Re{} and Im{} are real and
        # imaginary parts, and ap/m represent the complex constituent 
        # amplitudes for positive and negative frequencies when the input
        # is bivariate white noise. For a flat residual spectrum this works 
        # fine.
        # This is adapted here for "locally white" conditions, but I'm still
        # not sure how to handle a complex sxy, so this is set to zero
        # right now.
        p = (Pxrave[(k -1)] + Pxiave[(k -1)]) / 2
        d = (Pxrave[(k -1)] - Pxiave[(k -1)]) / 2
        sxy = Pxcave[(k -1)]
        B = np.hstack([p, 0, d, sxy, 0, p, sxy, - d, d, sxy, p, 0, sxy, - d, 0, p]).reshape(4,4)
        # Compute the transformation matrix that takes uncorrelated white 
        # noise and makes noise with the same statistical structure as the 
        # Fourier transformed noise.
        V, D = np.linalg.eig(B) # nargout=2
        Mat[:, :, (k -1)] = np.dot(V, np.diag(np.sqrt(np.diag(D))))
    # Generate realizations for the different analyzed constituents.
    N = np.zeros(shape=(4, nreal), dtype='float64')
    NM = np.zeros(shape=(max(fu.shape), nreal), dtype='float64')
    NP = NM

    for k in range(0, fu.shape[0]):
        l = np.flatnonzero(np.all([fu[k] > fband[:, 0] , fu[k] < fband[:, 1]],axis=0))
        N = np.hstack([np.zeros(shape=(4, 1), dtype='float64'), np.dot(np.squeeze(Mat[:, :, l]), np.random.randn(4, nreal-1))])
        NP[(k), :] = N[0, :] + 1j*N[1, :]
        NM[(k), :] = N[2, :] + 1j*N[3, :]
    return NP, NM
def noise_stats(xres, fu, dt):
    """NOISE_STATS: Computes statistics of residual energy for all 
     constituents (ignoring any cross-correlations between real and
     imaginary parts).
     S. Lentz  10/28/99
     R. Pawlowicz 11/1/00
     Version 1.0
    """
    fband, Pxrave, Pxiave, Pxcave = residual_spectrum(xres, fu, dt) # nargout=4
    nfband = fband.shape[0]
    mu = max(fu.shape)
    # Get the statistics for each component.
    ercx = np.zeros(shape=(mu, 1), dtype='float64')
    eicx = np.zeros(shape=(mu, 1), dtype='float64')
    for k1 in range(1, (nfband +1)):
        k = np.flatnonzero(fu >= fband[(k1 -1), 0] & fu <= fband[(k1 -1), 1])
        ercx[(k -1)] = sqrt(Pxrave[(k1 -1)])
        eicx[(k -1)] = sqrt(Pxiave[(k1 -1)])
    return ercx, eicx
def residual_spectrum(xres, fu, dt):
    """RESIDUAL_SPECTRUM: Computes statistics from an input spectrum over
     a number of bands, returning the band limits and the estimates for
     power spectra for real and imaginary parts and the cross-spectrum.          

     Mean values of the noise spectrum are computed for the following 
     8 frequency bands defined by their center frequency and band width:
     M0 +.1 cpd; M1 +-.2 cpd; M2 +-.2 cpd; M3 +-.2 cpd; M4 +-.2 cpd; 
     M5 +-.2 cpd; M6 +-.21 cpd; M7 (.26-.29 cpd); and M8 (.30-.50 cpd). 
     S. Lentz  10/28/99
     R. Pawlowicz 11/1/00
     Version 1.0
     Define frequency bands for spectral averaging.
    """
    fband = np.array([0.0001, 0.00417, 0.03192, 0.04859, 0.07218, 0.08884, 0.11243, 0.1291, 0.15269, 0.16936, 0.19295, 0.20961, 0.2332, 0.251, 0.26, 0.29, 0.3, 0.5]).reshape(9,2)
    # If we have a sampling interval> 1 hour, we might have to get
    # rid of some bins.
    #fband(fband(:,1)>1/(2*dt),:)=[];
    nfband = fband.shape[0]
    nx = max(xres.shape)
    # Spectral estimate (takes real time series only).
    # Matlab has changed their spectral estimator functions
    # To match the old code, I have to divide by 2*dt. This is because
    # 
    #  PSD*dt  is two-sided spectrum in units of power per hertz.
    #
    #  PWELCH is the one-sided spectrum in power per hertz
    #
    #  So PWELCH/2 = PSD*dt
    #[Pxr,fx]=psd(real(xres),nx,1/dt); 
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme. If you have an error here you are probably missing this toolbox
    #[Pxi,fx]=psd(imag(xres),nx,1/dt); 
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme.
    #[Pxc,fx]=csd(real(xres),imag(xres),nx,1/dt); 
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme.
    Pxr, fx = sps.welch(np.real(xres), window=np.hanning(nx), noverlap=np.ceil(nx / 2),nfft=nx,fs=1/dt) # nargout=2
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme. If you have an error here you are probably missing this toolbox
    Pxr = Pxr / 2 / dt
    Pxi, fx = sps.welch(np.imag(xres), window=np.hanning(nx), noverlap=np.ceil(nx / 2),nfft=nx,fs=1/dt) # nargout=2
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme.
    Pxi = Pxi / 2 / dt
    Pxc, fx = mplm.csd(np.real(xres), np.imag(xres),nx,1 / dt) # nargout=2
    # Call to SIGNAL PROCESSING TOOLBOX - see note in t_readme.
    Pxc = Pxc / 2 / dt
    df = fx[2] - fx[1]
    Pxr[np.around(fu / df).astype(int) ] = np.nan
    # Sets Px=NaN in bins close to analyzed frequencies
    Pxi[np.around(fu / df).astype(int)] = np.nan
    # (to prevent leakage problems?).
    Pxc[np.around(fu / df).astype(int)] = np.nan
    Pxrave = np.zeros(shape=(nfband, 1), dtype='float64')
    Pxiave = np.zeros(shape=(nfband, 1), dtype='float64')
    Pxcave = np.zeros(shape=(nfband, 1), dtype='float64')
    # Loop downwards in frequency through bands (cures short time series
    # problem with no data in lowest band).
    #
    # Divide by nx to get power per frequency bin, and multiply by 2
    # to account for positive and negative frequencies.
    #
    for k in range(nfband-1, -1, - 1):
        jband = np.flatnonzero(np.all(np.vstack([fx >= fband[(k), 0],fx <= fband[(k), 1] , np.isfinite(Pxr)]).T,axis=1))        
        if any(jband):
            Pxrave[k] = np.dot(np.mean(Pxr[(jband)]), 2) / nx
            Pxiave[k] = np.dot(np.mean(Pxi[(jband)]), 2) / nx
            Pxcave[k] = np.dot(np.mean(Pxc[(jband)]), 2) / nx
        else:
            if k < nfband:
                Pxrave[k] = Pxrave[(k + 1)]
                # Low frequency bin might not have any points...
                Pxiave[k] = Pxiave[(k + 1)]
                Pxcave[k] = Pxcave[(k + 1)]
    return fband, Pxrave, Pxiave, Pxcave
def errell(cxi, sxi, ercx, ersx, ercy, ersy):
    """[emaj,emin,einc,epha]=errell(cx,sx,cy,sy,ercx,ersx,ercy,ersy) computes
     the uncertainities in the ellipse parameters based on the 
     uncertainities in the least square fit cos,sin coefficients.

      INPUT:  cx,sx=cos,sin coefficients for x 
              cy,sy=cos,sin coefficients for y
              ercx,ersx=errors in x cos,sin coefficients
              ercy,ersy=errors in y cos,sin coefficients

      OUTPUT: emaj=major axis error
              emin=minor axis error
              einc=inclination error (deg)
              epha=pha error (deg)
     based on linear error propagation, with errors in the coefficients 
     cx,sx,cy,sy uncorrelated. 
     B. Beardsley  1/15/99; 1/20/99
     Version 1.0
    """
    r2d = 180.0 / pi
    cx = real(cxi[:])
    sx = real(sxi[:])
    cy = imag(cxi[:])
    sy = imag(sxi[:])
    ercx = ercx[:]
    ersx = ersx[:]
    ercy = ercy[:]
    ersy = ersy[:]
    rp = 0.5 * sqrt((cx + sy) ** 2 + (cy - sx) ** 2)
    rm = 0.5 * sqrt((cx - sy) ** 2 + (cy + sx) ** 2)
    ercx2 = ercx ** 2
    ersx2 = ersx ** 2
    ercy2 = ercy ** 2
    ersy2 = ersy ** 2
    # major axis error
    ex = (cx + sy) / rp
    fx = (cx - sy) / rm
    gx = (sx - cy) / rp
    hx = (sx + cy) / rm
    dcx2 = (0.25 * (ex + fx)) ** 2
    dsx2 = (0.25 * (gx + hx)) ** 2
    dcy2 = (0.25 * (hx - gx)) ** 2
    dsy2 = (0.25 * (ex - fx)) ** 2
    emaj = sqrt(dcx2 * ercx2 + dsx2 * ersx2 + dcy2 * ercy2 + dsy2 * ersy2)
    # minor axis error
    dcx2 = (0.25 * (ex - fx)) ** 2
    dsx2 = (0.25 * (gx - hx)) ** 2
    dcy2 = (0.25 * (hx + gx)) ** 2
    dsy2 = (0.25 * (ex + fx)) ** 2
    emin = sqrt(dcx2 * ercx2 + dsx2 * ersx2 + dcy2 * ercy2 + dsy2 * ersy2)
    # inclination error
    rn = np.dot(2.0, (cx * cy + sx * sy))
    rd = cx ** 2 + sx ** 2 - (cy ** 2 + sy ** 2)
    den = rn ** 2 + rd ** 2
    dcx2 = ((rd * cy - rn * cx) / den) ** 2
    dsx2 = ((rd * sy - rn * sx) / den) ** 2
    dcy2 = ((rd * cx + rn * cy) / den) ** 2
    dsy2 = ((rd * sx + rn * sy) / den) ** 2
    einc = r2d * sqrt(dcx2 * ercx2 + dsx2 * ersx2 + dcy2 * ercy2 + dsy2 * ersy2)
    # phase error
    rn = np.dot(2.0, (cx * sx + cy * sy))
    rd = cx ** 2 - sx ** 2 + cy ** 2 - sy ** 2
    den = rn ** 2 + rd ** 2
    dcx2 = ((rd * sx - rn * cx) / den) ** 2
    dsx2 = ((rd * cx + rn * sx) / den) ** 2
    dcy2 = ((rd * sy - rn * cy) / den) ** 2
    dsy2 = ((rd * cy + rn * sy) / den) ** 2
    epha = r2d * sqrt(dcx2 * ercx2 + dsx2 * ersx2 + dcy2 * ercy2 + dsy2 * ersy2)
    return emaj, emin, einc, epha
